======================================================================
RECORDING SCRIPT: LLM Inference: How KV Caching Makes AI Fast
======================================================================

Instructions:
1. Record each scene as a separate audio file
2. Name files by scene_id (e.g., scene1_hook.mp3)
3. Speak naturally - aim for conversational tone
4. Leave ~0.5s silence at start and end of each recording

----------------------------------------------------------------------

=== Scene 1: scene1_hook ===
Title: The Speed Problem
Words: 35 (~14 seconds)
Output file: scene1_hook.mp3

"Forty tokens per second. That's what you get with naive LLM inference. The best production systems? Over three thousand. Same model, same hardware—eighty-seven times faster. The difference is purely software. Here's how they do it."

----------------------------------------------------------------------

=== Scene 2: scene2_phases ===
Title: The Two Phases
Words: 53 (~21 seconds)
Output file: scene2_phases.mp3

"LLM inference has two distinct phases. First, the prefill phase processes your entire prompt in parallel. The GPU loves this - it can crunch all tokens at once. Then comes the decode phase, generating one token at a time. Each new token depends on the previous one. This is where the bottleneck hides."

----------------------------------------------------------------------

=== Scene 3: scene3_bottleneck ===
Title: The Decode Bottleneck
Words: 51 (~20 seconds)
Output file: scene3_bottleneck.mp3

"During decode, something surprising happens. The GPU sits mostly idle, waiting for data. Why? Because we're not limited by compute power. We're limited by memory bandwidth. The model weights are massive - billions of parameters. Moving them from memory to GPU takes time. And we do this for every single token."

----------------------------------------------------------------------

=== Scene 4: scene4_attention ===
Title: Understanding Attention
Words: 51 (~20 seconds)
Output file: scene4_attention.mp3

"Quick attention refresher. Each token produces Query, Key, and Value vectors. To predict the next token, we compute attention: Q times K-transpose, scaled, then softmax, then weighted sum of Values. Here's the key insight for this video: Keys and Values for past tokens never change. So why recompute them every time?"

----------------------------------------------------------------------

=== Scene 5: scene5_redundancy ===
Title: The Redundancy Problem
Words: 51 (~20 seconds)
Output file: scene5_redundancy.mp3

"Here's the first problem with naive decoding. For each new token, we recompute Keys and Values for ALL previous tokens. Token one? Compute once. Token two? Compute everything twice. Token one hundred? One hundred times the work. This is O of n squared complexity. Most of this computation is completely redundant."

----------------------------------------------------------------------

=== Scene 6: scene6_static_batching ===
Title: The Static Batching Problem
Words: 60 (~24 seconds)
Output file: scene6_static_batching.mp3

"The second problem: static batching. To make GPUs efficient, we process multiple requests together. But what happens when requests finish at different times? The short request waits for the long one. If one user asks 'what is two plus two' and another wants a five hundred word essay, the first user waits for the entire essay. Empty slots waste compute."

----------------------------------------------------------------------

=== Scene 7: scene7_memory_fragmentation ===
Title: The Memory Fragmentation Problem
Words: 52 (~21 seconds)
Output file: scene7_memory_fragmentation.mp3

"The third problem: memory fragmentation. We don't know how long responses will be, so we pre-allocate memory for the maximum length. A four thousand token buffer for every request. But most responses are short. We're using just twenty percent of allocated memory. The rest sits wasted, fragmenting GPU memory into unusable chunks."

----------------------------------------------------------------------

=== Scene 8: scene8_vllm_intro ===
Title: Introducing vLLM
Words: 38 (~15 seconds)
Output file: scene8_vllm_intro.mp3

"Now let's see how modern inference engines solve these problems. Three key techniques, now industry standard: KV caching, continuous batching, and PagedAttention. These ideas were pioneered in systems like vLLM and are now everywhere. Let's break them down."

----------------------------------------------------------------------

=== Scene 9: scene9_kvcache ===
Title: The KV Cache Solution
Words: 68 (~27 seconds)
Output file: scene9_kvcache.mp3

"Watch what happens with the KV cache. Token one generates Key-one and Value-one. These go straight into the cache. Now token two arrives. It generates Key-two and Value-two, but for attention, it reuses Key-one and Value-one from the cache. No recalculation. Token three? It reuses everything—Key-one, Value-one, Key-two, Value-two—all from the cache. Each token adds one new pair. The cache grows, but the work per token stays constant."

----------------------------------------------------------------------

=== Scene 10: scene10_mechanics ===
Title: How Attention Uses the Cache
Words: 51 (~20 seconds)
Output file: scene10_mechanics.mp3

"Here's the math behind it. The new token's Query multiplies against all cached Keys. Softmax turns those into attention weights. Then we take a weighted sum of cached Values. The result? Same output as recomputing everything, but at a fraction of the cost. Just matrix multiplies against tensors already in memory."

----------------------------------------------------------------------

=== Scene 11: scene11_continuous_batching ===
Title: Continuous Batching
Words: 89 (~36 seconds)
Output file: scene11_continuous_batching.mp3

"The second solution: continuous batching. First, what's a slot? A slot is a fixed memory allocation for one sequence in the GPU batch. Here we have four slots in our GPU. Watch what happens when Sequence A finishes. Its slot becomes available immediately, and a new sequence enters right away. This is the key innovation. Static batching waits for ALL sequences to finish before accepting new ones. Continuous batching fills empty slots immediately. New sequences enter available slots while others are still processing. The GPU stays at maximum utilization."

----------------------------------------------------------------------

=== Scene 12: scene12_paged_attention ===
Title: PagedAttention
Words: 58 (~23 seconds)
Output file: scene12_paged_attention.mp3

"The third solution: PagedAttention. Inspired by operating system virtual memory, we divide the KV cache into fixed-size blocks. Instead of pre-allocating one giant buffer, we allocate blocks on demand as tokens are generated. When a request finishes, its blocks return to a free list. No pre-allocation, no fragmentation. Memory utilization jumps from twenty percent to over ninety-five percent."

----------------------------------------------------------------------

=== Scene 13: scene13_more_optimizations ===
Title: More Optimizations Coming
Words: 26 (~10 seconds)
Output file: scene13_more_optimizations.mp3

"Those three techniques solve the serving bottleneck. But we're still leaving performance on the table. Two more optimizations push throughput even further: quantization and speculative decoding."

----------------------------------------------------------------------

=== Scene 14: scene14_quantization ===
Title: Quantization
Words: 55 (~22 seconds)
Output file: scene14_quantization.mp3

"We can push further with quantization. Model weights in sixteen-bit floating point use two bytes per parameter. Quantization compresses them to eight bits or even four bits, with minimal accuracy loss. Since decode is memory-bound, smaller weights mean faster loading. INT4 weights are four times smaller than FP16. That's up to four times faster inference."

----------------------------------------------------------------------

=== Scene 15: scene15_speculative_decoding ===
Title: Speculative Decoding
Words: 62 (~25 seconds)
Output file: scene15_speculative_decoding.mp3

"Here's a counterintuitive idea: use a smaller draft model to guess multiple tokens ahead, then verify them in parallel with the large model. For easy, predictable tokens, the draft is usually right. We accept all guesses and generate multiple tokens for the cost of one. For harder tokens, we fall back to the large model. Result: two to three times faster latency."

----------------------------------------------------------------------

=== Scene 16: scene16_scaling ===
Title: Scaling to Millions
Words: 54 (~22 seconds)
Output file: scene16_scaling.mp3

"One GPU isn't enough for production scale. Tensor parallelism shards each layer across GPUs—essential when a single model doesn't fit in memory. Pipeline parallelism chains GPUs in sequence, overlapping computation across micro-batches. And at massive scale, you deploy thousands of replicas behind load balancers, with smart routing to maximize cache reuse across similar requests."

----------------------------------------------------------------------

=== Scene 17: scene17_economics ===
Title: The Economics of Scale
Words: 61 (~24 seconds)
Output file: scene17_economics.mp3

"Let's talk real numbers. One million users, fifty tokens per second each. That's fifty million tokens per second total. With optimized inference at two thousand tokens per GPU-second, you need twenty-five thousand GPUs. At two dollars per GPU-hour, that's thirty-six million dollars per month. Every optimization in this video directly reduces that bill. A two-times throughput improvement halves your infrastructure cost."

----------------------------------------------------------------------

=== Scene 18: scene18_conclusion ===
Title: The Full Picture
Words: 85 (~34 seconds)
Output file: scene18_conclusion.mp3

"From forty tokens per second to over three thousand five hundred. An eighty-seven times improvement through pure software optimization. KV caching eliminates redundant computation. PagedAttention maximizes memory utilization. Quantization shrinks model size. Speculative decoding accelerates generation. Smart scaling handles millions of users. The key insight: LLM inference is memory-bound, not compute-bound. Every technique here serves one goal—maximize useful work per byte transferred. These optimizations power ChatGPT, Claude, and Gemini today. And inference costs still dominate AI budgets. The next ten-x improvement? That's an open problem."

----------------------------------------------------------------------

TOTAL: 18 scenes, 1000 words
Estimated recording time: 400 seconds (6.7 minutes)